# toxic_comment_identification

Dataset : https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge

Developed a bidirectional LSTM model that classifies whether a commect is toxic or not.  Identifies the following 6 labels in the comment
'toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate'


Implemented a simple interface using Gradio
<img width="822" alt="image" src="https://github.com/pranamya18/toxic_comment_identification/assets/49710041/8332e29a-8505-4ca3-acf8-a5b3a761cfc2">
