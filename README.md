# toxic_comment_identification

Dataset : https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge

Developed a bidirectional LSTM model that classifies whether a commect is toxic or not.  Identifies the following 6 labels in the comment
'toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate'


Implemented a simple interface using Gradio

<img width="822" alt="Screenshot 2023-06-27 234503" src="https://github.com/pranamya18/toxic_comment_identification/assets/49710041/8dfacb9d-679a-4101-ab1c-590e8367aee1">
